Creating a real-time image recognition system on edge devices using Python involves utilizing deep learning models, libraries like OpenCV for image processing, and leveraging NVIDIA's GPU capabilities. 
Here's a simplified example using a pre-trained model to perform real-time object detection:

```python
import cv2
import numpy as np

# Load pre-trained YOLO model for object detection
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")

# Load COCO class labels
classes = []
with open("coco.names", "r") as f:
    classes = f.read().strip().split("\n")

# Set GPU as backend for OpenCV's deep learning module
net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)

# Open a video capture stream (webcam, for example)
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()

    # Perform object detection
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    # Process detection results
    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]

            if confidence > 0.5:
                # Object detected
                label = classes[class_id]
                cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    # Display the frame
    cv2.imshow("Real-Time Object Detection", frame)
    
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
```

Please note that this is a simplified example. In a real-world project, you would need to fine-tune the model, handle the integration with edge devices, and ensure efficient GPU utilization.
Additionally, you'd need to use a trained model (`.weights` and `.cfg` files) and class labels (`.names` file) suitable for your use case.
